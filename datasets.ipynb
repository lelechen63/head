{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import librosa\n",
    "import time\n",
    "import copy\n",
    "import python_speech_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Voxceleb_head_movements_derivative(data.Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset_dir,\n",
    "                 train='train'):\n",
    "        self.train = train\n",
    "        self.num_frames = 64  \n",
    "        self.root  = '/data2/lchen63/voxceleb/'\n",
    "                 \n",
    "        if self.train=='train':\n",
    "            _file = open(os.path.join(dataset_dir, \"train.pkl\"), \"rb\")\n",
    "            self.data = pickle.load(_file)\n",
    "            _file.close()\n",
    "        elif self.train =='test':\n",
    "            _file = open(os.path.join(dataset_dir, \"test.pkl\"), \"rb\")\n",
    "            self.data = pickle.load(_file)\n",
    "            _file.close()\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            tmp = self.data[index][0].split('/')\n",
    "            mean = np.load( os.path.join(self.root, 'unzip', self.data[index][0],'mean.npy' ))\n",
    "            if len(self.data[index][2]) ==1:\n",
    "                landmark = np.load( os.path.join(self.root, 'unzip', self.data[index][0], self.data[index][2][0] + '.npy' ))\n",
    "                middle = int(self.data[index][1] / 2)\n",
    "                in_max = middle - 64\n",
    "                if (in_max <= 0):\n",
    "                    in_max = 10\n",
    "                out_max = self.data[index][1] - 64\n",
    "                if out_max < middle:\n",
    "                    tmp = out_max\n",
    "                    out_max = middle\n",
    "                    middle = tmp\n",
    "                elif out_max == middle:\n",
    "                    middle -= 5\n",
    "                in_start  = random.choice([x for x in range(0,in_max)])\n",
    "                out_start = random.choice([x for x in range(middle,out_max)])\n",
    "                in_lmark = landmark[in_start:in_start+ 64] - mean\n",
    "                out_lmark = landmark[out_start:out_start+ 64] - mean\n",
    "            else:\n",
    "                in_landmark = np.load( os.path.join(self.root, 'unzip', self.data[index][0], self.data[index][2][0] + '.npy' ))\n",
    "                r  = random.choice([x for x in range(1,len(self.data[index][2]))])\n",
    "                out_landmark = np.load( os.path.join(self.root, 'unzip', self.data[index][0], self.data[index][2][r] + '.npy' ))\n",
    "                in_max = in_landmark.shape[0] - 64\n",
    "                out_max = out_landmark.shape[0] - 64\n",
    "                in_start  = random.choice([x for x in range(0,in_max)])\n",
    "                out_start = random.choice([x for x in range(0,out_max)])                  \n",
    "                in_lmark = in_landmark[in_start:in_start + 64] - mean\n",
    "                out_lmark = out_landmark[out_start:out_start + 64] - mean\n",
    "                  \n",
    "            in_lmark = torch.FloatTensor(in_lmark)\n",
    "            out_lmark = torch.FloatTensor(out_lmark)\n",
    "            mean = torch.FloatTensor(mean)        \n",
    "            return in_lmark, out_lmark, mean\n",
    "               \n",
    "    def __len__(self):\n",
    "        \n",
    "            return len(self.data)\n",
    "\n",
    "class LRWdataset1D_single_gt(data.Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset_dir,\n",
    "                 output_shape=[128, 128],\n",
    "                 train='train'):\n",
    "        self.train = train\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.output_shape = tuple(output_shape)\n",
    "\n",
    "        if not len(output_shape) in [2, 3]:\n",
    "            raise ValueError(\"[*] output_shape must be [H,W] or [C,H,W]\")\n",
    "\n",
    "        if self.train=='train':\n",
    "            _file = open(os.path.join(dataset_dir, \"new_img_full_gt_train.pkl\"), \"rb\")\n",
    "            self.train_data = pickle.load(_file)\n",
    "            _file.close()\n",
    "        elif self.train =='test':\n",
    "            _file = open(os.path.join(dataset_dir, \"new_img_full_gt_test.pkl\"), \"rb\")\n",
    "            self.test_data = pickle.load(_file)\n",
    "            _file.close()\n",
    "        elif self.train =='demo' :\n",
    "            _file = open(os.path.join(dataset_dir, \"new_img_full_gt_demo.pkl\"), \"rb\")\n",
    "            self.demo_data = pickle.load(_file)\n",
    "            _file.close()\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # In training phase, it return real_image, wrong_image, text\n",
    "        if self.train=='train':\n",
    "\n",
    "                #load righ img\n",
    "                image_path = '../dataset/regions/' +  self.train_data[index][0]\n",
    "                landmark_path = '../dataset/landmark1d/' + self.train_data[index][0][:-8] + '.npy'\n",
    "\n",
    "                landmark = np.load(landmark_path) * 5.0\n",
    "\n",
    "                right_landmark = landmark[self.train_data[index][1] - 1]\n",
    "                right_landmark = torch.FloatTensor(right_landmark.reshape(-1))\n",
    "                \n",
    "                im = cv2.imread(image_path)\n",
    "                im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "                im = cv2.resize(im, self.output_shape)\n",
    "                im = self.transform(im)\n",
    "                right_img = torch.FloatTensor(im)\n",
    "\n",
    "                r = random.choice(\n",
    "                    [x for x in range(1,30)])\n",
    "                example_path =   image_path[:-8] + '_%03d.jpg'%r\n",
    "                example_landmark = landmark[r - 1]\n",
    "                example_landmark = torch.FloatTensor(example_landmark.reshape(-1))\n",
    "\n",
    "                example_img = cv2.imread(example_path)\n",
    "                example_img = cv2.cvtColor(example_img, cv2.COLOR_BGR2RGB)\n",
    "                example_img = cv2.resize(example_img, self.output_shape)\n",
    "                example_img = self.transform(example_img)\n",
    "\n",
    "                return example_img, example_landmark, right_img,right_landmark\n",
    "\n",
    "        elif self.train =='test':\n",
    "            # try:\n",
    "                #load righ img\n",
    "            image_path = '../dataset/regions/' +  self.test_data[index][0]\n",
    "            landmark_path = '../dataset/landmark1d/' + self.test_data[index][0][:-8] + '.npy'\n",
    "            landmark = np.load(landmark_path) * 5.0\n",
    "            right_landmark = landmark[self.test_data[index][1] - 1]\n",
    "            \n",
    "            right_landmark = torch.FloatTensor(right_landmark.reshape(-1))\n",
    "            \n",
    "            im = cv2.imread(image_path)\n",
    "            \n",
    "            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "            im = cv2.resize(im, self.output_shape)\n",
    "            im = self.transform(im)\n",
    "            right_img = torch.FloatTensor(im)\n",
    "            \n",
    "            example_path =   '../image/musk1_region.jpg'\n",
    "            example_landmark = np.load('../image/musk1.npy')\n",
    "            \n",
    "            example_landmark = torch.FloatTensor(example_landmark.reshape(-1)) * 5.0\n",
    "\n",
    "            example_img = cv2.imread(example_path)\n",
    "            example_img = cv2.cvtColor(example_img, cv2.COLOR_BGR2RGB)\n",
    "            example_img = cv2.resize(example_img, self.output_shape)\n",
    "            example_img = self.transform(example_img)\n",
    "\n",
    "            return example_img, example_landmark, right_img,right_landmark\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ -2.3184,  -3.1692,  -2.6494],\n",
      "          [ -0.2537,  -2.6517,  -3.8363],\n",
      "          [  0.8955,  -1.6766,  -4.6313],\n",
      "          ...,\n",
      "          [  0.5473,  -3.1542,   0.2111],\n",
      "          [  0.0597,  -1.6269,  -0.1522],\n",
      "          [ -1.8706,  -2.1045,  -0.5340]],\n",
      "\n",
      "         [[ -2.3184,  -1.1692,  -2.7177],\n",
      "          [ -0.2537,  -2.6517,  -3.9134],\n",
      "          [  0.8955,  -1.6766,  -4.8237],\n",
      "          ...,\n",
      "          [ -2.4527,  -1.1542,  -0.7165],\n",
      "          [ -2.9403,  -1.6269,  -1.0698],\n",
      "          [ -1.8706,  -0.1045,  -1.3979]],\n",
      "\n",
      "         [[  0.6816,   0.8308,  -1.2617],\n",
      "          [  0.7463,  -0.6517,  -2.3899],\n",
      "          [  1.8955,   0.3234,  -3.1877],\n",
      "          ...,\n",
      "          [ -1.4527,  -1.1542,  -0.3343],\n",
      "          [ -1.9403,  -1.6269,  -0.4964],\n",
      "          [ -3.8706,  -0.1045,  -0.9219]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ -1.3184, -21.1692,  -2.8887],\n",
      "          [ -5.2537, -19.6517,  -2.2289],\n",
      "          [-10.1045, -16.6766,  -1.6845],\n",
      "          ...,\n",
      "          [-12.4527,  -0.1542,   3.3372],\n",
      "          [-12.9403,  -0.6269,   3.4458],\n",
      "          [-12.8706,  -3.1045,   3.4162]],\n",
      "\n",
      "         [[  0.6816, -23.1692,  -3.7846],\n",
      "          [ -5.2537, -21.6517,  -2.3210],\n",
      "          [-12.1045, -20.6766,  -0.9436],\n",
      "          ...,\n",
      "          [-15.4527,   0.8458,   4.2113],\n",
      "          [-15.9403,  -1.6269,   4.4307],\n",
      "          [-15.8706,  -4.1045,   4.3543]],\n",
      "\n",
      "         [[  0.6816, -25.1692,  -1.4425],\n",
      "          [ -7.2537, -23.6517,  -0.2708],\n",
      "          [-12.1045, -22.6766,   0.7686],\n",
      "          ...,\n",
      "          [-15.4527,   0.8458,   2.9047],\n",
      "          [-15.9403,  -1.6269,   3.4594],\n",
      "          [-15.8706,  -4.1045,   3.6148]]],\n",
      "\n",
      "\n",
      "        [[[  3.4554,  18.4851,  -9.3614],\n",
      "          [  4.9257,  18.0594,  -8.1028],\n",
      "          [  7.9307,  17.0792,  -6.8240],\n",
      "          ...,\n",
      "          [ -3.4059,  -1.2178,   3.0687],\n",
      "          [ -3.6089,   0.8911,   2.2108],\n",
      "          [ -2.8366,   0.9208,   1.0977]],\n",
      "\n",
      "         [[  2.4554,  21.4851,  -8.9299],\n",
      "          [  3.9257,  21.0594,  -7.0981],\n",
      "          [  6.9307,  21.0792,  -5.3632],\n",
      "          ...,\n",
      "          [ -1.4059,   1.7822,   3.0320],\n",
      "          [ -2.6089,   2.8911,   2.6299],\n",
      "          [ -1.8366,   2.9208,   1.7201]],\n",
      "\n",
      "         [[  1.4554,  19.4851,  -9.4411],\n",
      "          [  3.9257,  20.0594,  -7.6194],\n",
      "          [  5.9307,  19.0792,  -5.8573],\n",
      "          ...,\n",
      "          [ -3.4059,  -2.2178,   3.6627],\n",
      "          [ -3.6089,  -1.1089,   3.0662],\n",
      "          [ -3.8366,   1.9208,   2.2419]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ -0.5446,  -0.5149,   7.2136],\n",
      "          [ -2.0743,   1.0594,   8.7417],\n",
      "          [ -5.0693,   1.0792,  10.0665],\n",
      "          ...,\n",
      "          [ -1.4059,   1.7822,   3.3723],\n",
      "          [ -1.6089,   0.8911,   3.7051],\n",
      "          [ -0.8366,  -1.0792,   4.3172]],\n",
      "\n",
      "         [[ -0.5446,  -3.5149,   7.3425],\n",
      "          [ -4.0743,  -1.9406,   8.6970],\n",
      "          [ -5.0693,  -0.9208,   9.8972],\n",
      "          ...,\n",
      "          [  1.5941,   1.7822,   3.6895],\n",
      "          [  1.3911,   0.8911,   3.9772],\n",
      "          [  2.1634,  -1.0792,   4.6229]],\n",
      "\n",
      "         [[ -0.5446,  -2.5149,   8.7212],\n",
      "          [ -4.0743,  -1.9406,   9.8280],\n",
      "          [ -7.0693,  -0.9208,  10.8514],\n",
      "          ...,\n",
      "          [  0.5941,  -0.2178,   4.8272],\n",
      "          [  2.3911,  -1.1089,   5.3601],\n",
      "          [  1.1634,  -1.0792,   6.1991]]]])\n",
      "=============\n",
      "torch.Size([2, 64, 68, 3])\n",
      "torch.Size([2, 64, 68, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Traceback (most recent call last):\n  File \"/data/lchen63/anaconda2/lib/python2.7/site-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/data/lchen63/anaconda2/lib/python2.7/site-packages/torch/utils/data/_utils/collate.py\", line 68, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/data/lchen63/anaconda2/lib/python2.7/site-packages/torch/utils/data/_utils/collate.py\", line 43, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 64 and 57 in dimension 1 at /opt/conda/conda-bld/pytorch_1556653194318/work/aten/src/TH/generic/THTensor.cpp:711\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-e3003df6be1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                          \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                          shuffle=True, drop_last=True)\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0min_lmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/lchen63/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/lchen63/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KeyError:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Traceback (most recent call last):\n  File \"/data/lchen63/anaconda2/lib/python2.7/site-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/data/lchen63/anaconda2/lib/python2.7/site-packages/torch/utils/data/_utils/collate.py\", line 68, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/data/lchen63/anaconda2/lib/python2.7/site-packages/torch/utils/data/_utils/collate.py\", line 43, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 64 and 57 in dimension 1 at /opt/conda/conda-bld/pytorch_1556653194318/work/aten/src/TH/generic/THTensor.cpp:711\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = Voxceleb_head_movements_derivative( '/data2/lchen63/voxceleb/txt', 'train')\n",
    "data_loader = DataLoader(dataset,\n",
    "                         batch_size=2,\n",
    "                         num_workers=1,                          \n",
    "                         shuffle=True, drop_last=True)\n",
    "for step, (in_lmark, out_lmark, mean) in enumerate(data_loader):\n",
    "    if step == 1000:\n",
    "        break\n",
    "#     print (in_lmark.shape)\n",
    "#     print ( out_lmark.shape)\n",
    "    print (in_lmark)\n",
    "    if in_lmark.shape != [2,64,68,3] or out_lmark.shape[0:] != [2,64,68,3]:\n",
    "        print ('=============')\n",
    "        print (in_lmark.shape)\n",
    "        print (out_lmark.shape)\n",
    "#     print(mean.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-92c0b7c3475d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "tmp = []\n",
    "for i in range(128):\n",
    "    tmp.append(i)\n",
    "    \n",
    "    \n",
    "b = tmp - tmp[3:3+64]\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
